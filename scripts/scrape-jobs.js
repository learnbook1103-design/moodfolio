// scripts/scrape-jobs.js
// ì±„ìš©ê³µê³  ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸

import { JobScraper } from '../lib/scrapers/job-scraper.js';
import { DataCleaner } from '../lib/data-pipeline/cleaner.js';
import { DataAggregator } from '../lib/data-pipeline/aggregator.js';
import { supabase } from '../lib/supabase.js';

async function main() {
    const scraper = new JobScraper();
    const cleaner = new DataCleaner();
    const aggregator = new DataAggregator();

    const jobTypes = ['developer', 'designer', 'marketer', 'service'];
    const targetPerSource = 25; // ê° ì†ŒìŠ¤ì—ì„œ 25ê°œì”© = ì´ 50ê°œ

    console.log('ðŸš€ Starting job scraping...\n');

    for (const jobType of jobTypes) {
        console.log(`\nðŸ“‹ Processing ${jobType}...`);

        let allJobs = [];

        // 1. ì›í‹°ë“œì—ì„œ ìˆ˜ì§‘
        console.log(`  Scraping from Wanted...`);
        const wantedJobs = await scraper.scrapeWanted(jobType, targetPerSource);
        allJobs.push(...wantedJobs);

        // 2. ì‚¬ëžŒì¸ì—ì„œ ìˆ˜ì§‘
        console.log(`  Scraping from Saramin...`);
        const saraminJobs = await scraper.scrapeSaramin(jobType, targetPerSource);
        allJobs.push(...saraminJobs);

        // 3. ë°ì´í„° ì •ì œ
        console.log(`  Cleaning ${allJobs.length} jobs...`);
        const cleanedJobs = allJobs.map(job => cleaner.cleanJobPosting(job));

        // 4. ì¤‘ë³µ ì œê±°
        const uniqueJobs = cleaner.deduplicateJobs(cleanedJobs);
        console.log(`  After deduplication: ${uniqueJobs.length} jobs`);

        // 5. ìœ íš¨ì„± ê²€ì¦
        const validJobs = uniqueJobs.filter(job => {
            const validation = cleaner.validateJobPosting(job);
            if (!validation.isValid) {
                console.log(`  âš ï¸ Invalid job: ${validation.errors.join(', ')}`);
            }
            return validation.isValid;
        });

        console.log(`  Valid jobs: ${validJobs.length}`);

        // 6. Supabaseì— ì €ìž¥
        if (validJobs.length > 0) {
            const { data, error } = await supabase
                .from('job_postings')
                .upsert(validJobs, {
                    onConflict: 'job_id',
                    ignoreDuplicates: false
                });

            if (error) {
                console.error(`  âŒ Error saving to Supabase:`, error);
            } else {
                console.log(`  âœ… Saved ${validJobs.length} jobs to database`);
            }
        }

        // 7. ì¸ì‚¬ì´íŠ¸ ì§‘ê³„
        console.log(`  Aggregating insights...`);
        await aggregator.aggregateInsights(jobType);

        // 8. íŠ¸ë Œë“œ ê³„ì‚°
        console.log(`  Calculating trends...`);
        await aggregator.calculateWeeklyTrends(jobType);

        console.log(`  âœ… Completed ${jobType}\n`);

        // Rate limiting between job types
        await new Promise(resolve => setTimeout(resolve, 2000));
    }

    console.log('\nðŸŽ‰ All done! Summary:');

    // ìµœì¢… í†µê³„
    for (const jobType of jobTypes) {
        const { count } = await supabase
            .from('job_postings')
            .select('*', { count: 'exact', head: true })
            .eq('job_type', jobType);

        const { data: insights } = await supabase
            .from('market_insights_cache')
            .select('sample_size, data_quality_score')
            .eq('job_type', jobType)
            .single();

        console.log(`  ${jobType}: ${count} jobs, quality score: ${insights?.data_quality_score?.toFixed(2) || 'N/A'}`);
    }
}

// ì‹¤í–‰
main().catch(console.error);
